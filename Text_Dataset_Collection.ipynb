{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiDsiM0xHwgf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import nltk\n",
        "import string\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Define categories and sample websites\n",
        "categories = {\n",
        "    \"Technology\": [\"https://www.theverge.com/tech\", \"https://techcrunch.com/\", \"https://www.wired.com/category/tech/\"],\n",
        "    \"Science\": [\"https://www.sciencenews.org/\", \"https://www.scientificamerican.com/\", \"https://www.nature.com/\"]\n",
        "    # Add more categories and sources as needed, I added less for shorter runtime\n",
        "}\n",
        "\n",
        "# Directory to store text files\n",
        "os.makedirs(\"scraped_texts\", exist_ok=True)\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.replace(\"\\n\", \" \")  # Remove newlines\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    text = ' '.join(text.split())  # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "def scrape_articles(url):\n",
        "    try:\n",
        "        response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        # Extract all paragraphs\n",
        "        paragraphs = soup.find_all(\"p\")\n",
        "        content = ' '.join([p.get_text() for p in paragraphs])\n",
        "        return clean_text(content)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to scrape {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def save_text(category, text):\n",
        "    with open(f\"scraped_texts/{category}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(text)\n",
        "\n",
        "def summarize_text(text, num_sentences=5):\n",
        "    sentences = sent_tokenize(text)\n",
        "    if len(sentences) <= num_sentences:\n",
        "        return text  # Return full text if it's already short\n",
        "\n",
        "    # Vectorize sentences using TF-IDF\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    sentence_vectors = vectorizer.fit_transform(sentences)\n",
        "\n",
        "    # Compute similarity matrix\n",
        "    similarity_matrix = cosine_similarity(sentence_vectors, sentence_vectors)\n",
        "    scores = similarity_matrix.sum(axis=1)\n",
        "\n",
        "    # Get top-ranked sentences\n",
        "    ranked_sentences = [sentences[i] for i in np.argsort(scores)[-num_sentences:]]\n",
        "    return ' '.join(ranked_sentences)\n",
        "\n",
        "# Scrape and summarize\n",
        "for category, urls in categories.items():\n",
        "    print(f\"Scraping category: {category}...\")\n",
        "    all_text = \"\"\n",
        "    for url in urls:\n",
        "        text = scrape_articles(url)\n",
        "        all_text += text + \"\\n\\n\"\n",
        "    save_text(category, all_text)\n",
        "\n",
        "    # Summarize\n",
        "    summary = summarize_text(all_text)\n",
        "    print(f\"Summary for {category}:\\n\", summary, \"\\n\")\n"
      ]
    }
  ]
}